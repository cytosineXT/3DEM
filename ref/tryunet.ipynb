{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [4, 16, 4, 4], expected input[5, 32, 18, 18] to have 4 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m22500\u001b[39m, \u001b[38;5;241m576\u001b[39m)  \u001b[38;5;66;03m# 随机生成输入张量，范围 [0, 1]\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 80\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, n)\u001b[0m\n\u001b[1;32m     72\u001b[0m x_middle \u001b[38;5;241m=\u001b[39m x_middle\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), n, \u001b[38;5;241m45\u001b[39m, \u001b[38;5;241m90\u001b[39m)  \u001b[38;5;66;03m# 调整形状为 (batch, n, 45, 90)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# # 上采样部分\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# # 跳跃连接 1：skip3 -> 上采样后大小对齐\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# skip3_flat = skip3.view(skip3.size(0), -1)  # 展平 skip3\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# skip3_proj = self.fc_skip3(skip3_flat).view(x_middle.size())  # 映射到 (batch, n, 45, 90)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# x = x_middle + skip3_proj  # 跳跃连接\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, n*4, 90, 180)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [4, 16, 4, 4], expected input[5, 32, 18, 18] to have 4 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n=64):  # `n` 是 Unet 中间层通道数，默认为 64\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 1D卷积降维，从576降到1\n",
    "        self.conv1d_dim_reduce = nn.Conv1d(in_channels=576, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        # 下采样部分\n",
    "        self.down_conv1 = nn.Conv2d(1, n, kernel_size=4, stride=2, padding=1)  # [150, 150] -> [75, 75]\n",
    "        self.down_conv2 = nn.Conv2d(n, n * 2, kernel_size=4, stride=2, padding=1)  # [75, 75] -> [37, 37]\n",
    "        self.down_conv3 = nn.Conv2d(n * 2, n * 4, kernel_size=4, stride=2, padding=1)  # [37, 37] -> [18, 18]\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck_conv = nn.Conv2d(n * 4, n * 8, kernel_size=3, padding=1)  # [18, 18] -> [18, 18]\n",
    "        \n",
    "        # 全连接层，映射到目标大小\n",
    "        self.fc_down_to_middle = nn.Linear(18 * 18 * n * 8, 45 * 90 * n)\n",
    "        \n",
    "        # 上采样部分\n",
    "        self.upconv1 = nn.ConvTranspose2d(n, n * 4, kernel_size=4, stride=2, padding=1)  # [45, 90] -> [90, 180]\n",
    "        self.bn1 = nn.BatchNorm2d(n * 4)\n",
    "        self.conv1_1 = nn.Conv2d(n * 4, n * 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(n * 4)\n",
    "        self.conv1_2 = nn.Conv2d(n * 4, n * 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(n * 4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(n * 4, n * 2, kernel_size=4, stride=2, padding=1)  # [90, 180] -> [180, 360]\n",
    "        self.bn2 = nn.BatchNorm2d(n * 2)\n",
    "        self.conv2_1 = nn.Conv2d(n * 2, n * 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(n * 2)\n",
    "        self.conv2_2 = nn.Conv2d(n * 2, n * 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(n * 2)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(n * 2, 1, kernel_size=4, stride=2, padding=1)  # [180, 360] -> [360, 720]\n",
    "        self.bn3 = nn.BatchNorm2d(1)\n",
    "        self.conv3_1 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(1)\n",
    "        self.conv3_2 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        # 最终 1x1 卷积\n",
    "        self.conv1x1 = nn.Conv2d(1, 1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Skip connections 的线性映射\n",
    "        # self.fc_skip1 = nn.Linear(75 * 75 * n, 180 * 360 * n * 2)\n",
    "        # self.fc_skip2 = nn.Linear(37 * 37 * n * 2, 90 * 180 * n * 4)\n",
    "        # self.fc_skip3 = nn.Linear(18 * 18 * n * 4, 45 * 90 * n)\n",
    "\n",
    "    def forward(self, x, n):\n",
    "        # 1D卷积将维度从576降到1\n",
    "        x = x.permute(0, 2, 1)  # 重排为 (batch, dim, length)\n",
    "        x = self.conv1d_dim_reduce(x)  # (batch, 1, 22500)\n",
    "        x = x.permute(0, 2, 1)  # 重排回 (batch, length, dim)\n",
    "        x = x.view(x.size(0), 1, 150, 150)  # 重塑为 (batch, 1, 150, 150)\n",
    "        \n",
    "        # 下采样部分\n",
    "        skip1 = self.down_conv1(x)  # (batch, n, 75, 75)\n",
    "        skip2 = self.down_conv2(skip1)  # (batch, n*2, 37, 37)\n",
    "        skip3 = self.down_conv3(skip2)  # (batch, n*4, 18, 18)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck_conv(skip3)  # (batch, n*8, 18, 18)\n",
    "        \n",
    "        # 展平并通过全连接层，映射到中间变量大小\n",
    "        x_flat = x.view(x.size(0), -1)  # 展平为 (batch, 18*18*n*8)\n",
    "        x_middle = self.fc_down_to_middle(x_flat)  # 映射到 (batch, 45*90*n)\n",
    "        x_middle = x_middle.view(x.size(0), n, 45, 90)  # 调整形状为 (batch, n, 45, 90)\n",
    "        \n",
    "        # # 上采样部分\n",
    "        # # 跳跃连接 1：skip3 -> 上采样后大小对齐\n",
    "        # skip3_flat = skip3.view(skip3.size(0), -1)  # 展平 skip3\n",
    "        # skip3_proj = self.fc_skip3(skip3_flat).view(x_middle.size())  # 映射到 (batch, n, 45, 90)\n",
    "        # x = x_middle + skip3_proj  # 跳跃连接\n",
    "        \n",
    "        x = self.upconv1(x)  # (batch, n*4, 90, 180)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.bn1_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # # 跳跃连接 2：skip2 -> 上采样后大小对齐\n",
    "        # skip2_flat = skip2.view(skip2.size(0), -1)  # 展平 skip2\n",
    "        # skip2_proj = self.fc_skip2(skip2_flat).view(x.size(0), n * 4, 90, 180)  # 映射到 (batch, n*4, 90, 180)\n",
    "        # x = x + skip2_proj  # 跳跃连接\n",
    "        \n",
    "        x = self.upconv2(x)  # (batch, n*2, 180, 360)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # # 跳跃连接 3：skip1 -> 上采样后大小对齐\n",
    "        # skip1_flat = skip1.view(skip1.size(0), -1)  # 展平 skip1\n",
    "        # skip1_proj = self.fc_skip1(skip1_flat).view(x.size(0), n * 2, 180, 360)  # 映射到 (batch, n*2, 180, 360)\n",
    "        # x = x + skip1_proj  # 跳跃连接\n",
    "        \n",
    "        x = self.upconv3(x)  # (batch, 1, 360, 720)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 最终 1x1 卷积\n",
    "        x = self.conv1x1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "decoder = Decoder(n=4)\n",
    "x = torch.rand(5, 22500, 576)  # 随机生成输入张量，范围 [0, 1]\n",
    "y = decoder(x,n=4)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jxtnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
