{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0121GB\n",
      "torch.Size([1, 576, 22500]) 12960000\n",
      "耗时0.1643s\n",
      "torch.Size([22500, 1, 576]) 12960000\n",
      "耗时0.0032s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=576, nhead=4, dim_feedforward=256),num_layers=2).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "tic = time.time()\n",
    "input_matrix = torch.randn(1, 576, 22500).to(device)  # batchsize channel 长\n",
    "print(input_matrix.shape, 576 * 22500)\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n",
    "\n",
    "tic = time.time()\n",
    "output_image = input_matrix.reshape(22500,1,576) # Reshape to (seq_len, batch_size, input_channel)\n",
    "output_image = transformer_model(output_image)\n",
    "print(output_image.shape, output_image.shape[0] * output_image.shape[1] * output_image.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 22500]) 12960000\n",
      "模型占用0.6303GB\n",
      "耗时2.8123s\n",
      "torch.Size([1, 22500, 576]) 12960000\n",
      "耗时0.0009s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m耗时\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtic\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m output_image \u001b[38;5;241m=\u001b[39m \u001b[43mdit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_image\u001b[38;5;241m.\u001b[39mshape, output_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m output_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m output_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m耗时\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtic\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/3DEM/ditmodel.py:240\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, t, y)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, y):\n\u001b[1;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;124;03m    Forward pass of DiT.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m    x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    t: (N,) tensor of diffusion timesteps\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    y: (N,) tensor of class labels\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed  \u001b[38;5;66;03m# (N, T, D), where T = H * W / patch_size ** 2 T就是token长度 N是batchsize吧？我就不用embedder了，直接就是token\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2 T就是token长度 N是batchsize吧？\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jxtnet/lib/python3.9/site-packages/timm/layers/patch_embed.py:85\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 85\u001b[0m     B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict_img_size:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ditmodel import DiT\n",
    "from diffusion import create_diffusion\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "# input_matrix = torch.randn(1, 22500, 576).to(device)  # batchsize channel 长 (N, T, D)\n",
    "input_matrix = torch.randn(1, 576, 22500).to(device)  # batchsize channel 长\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")  # default: 1000 steps, linear noise schedule\n",
    "print(input_matrix.shape, 576 * 22500)\n",
    "dit = DiT(hidden_size=576, num_heads=16).to(device)\n",
    "get_model_memory_nolog(dit)\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "output_image = input_matrix.reshape(1,22500,576) # Reshape to (seq_len, batch_size, input_channel)  (N, T, D)\n",
    "t = torch.randint(0, diffusion.num_timesteps, (output_image.shape[0],), device=device)\n",
    "print(output_image.shape, output_image.shape[0] * output_image.shape[1] * output_image.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "output_image = dit(x=output_image,t=t,y=None)\n",
    "print(output_image.shape, output_image.shape[0] * output_image.shape[1] * output_image.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 2500]) 1440000\n",
      "模型占用0.6281GB\n",
      "耗时2.8092s\n",
      "torch.Size([1, 2500, 576]) 1440000\n",
      "耗时0.0045s\n",
      "torch.Size([1, 2500, 1]) 2500\n",
      "耗时0.1128s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ditmodel import DiT\n",
    "from diffusion import create_diffusion\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "length = 2500\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "# input_matrix = torch.randn(1, 22500, 576).to(device)  # batchsize channel 长 (N, T, D)\n",
    "input_matrix = torch.randn(1, 576, length).to(device)  # batchsize channel 长\n",
    "# input_matrix = torch.randn(1, 576, 22500).to(device)  # batchsize channel 长\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")  # default: 1000 steps, linear noise schedule\n",
    "print(input_matrix.shape, 576 * length)\n",
    "# print(input_matrix.shape, 576 * 22500)\n",
    "dit = DiT(hidden_size=576, num_heads=16, depth=28, length=length).to(device)\n",
    "get_model_memory_nolog(dit)\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "output_image = input_matrix.reshape(1,-1,576) # Reshape to (seq_len, batch_size, input_channel)  (N, T, D)\n",
    "t = torch.randint(0, diffusion.num_timesteps, (output_image.shape[0],), device=device)\n",
    "print(output_image.shape, output_image.shape[0] * output_image.shape[1] * output_image.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "output_image = dit(x=output_image,t=t,y=None)\n",
    "print(output_image.shape, output_image.shape[0] * output_image.shape[1] * output_image.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 10000]) 5760000\n",
      "模型占用0.6281GB\n",
      "耗时2.6495s\n",
      "torch.Size([1, 10000, 576]) 5760000\n",
      "耗时0.0036s\n",
      "torch.Size([1, 10000, 1]) 10000\n",
      "耗时1.5592s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ditmodel import DiT\n",
    "from diffusion import create_diffusion\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "length = 10000\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "# input_matrix = torch.randn(1, 22500, 576).to(device)  # batchsize channel 长 (N, T, D)\n",
    "input_matrix = torch.randn(1, 576, length).to(device)  # batchsize channel 长\n",
    "# input_matrix = torch.randn(1, 576, 22500).to(device)  # batchsize channel 长\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")  # default: 1000 steps, linear noise schedule\n",
    "print(input_matrix.shape, 576 * length)\n",
    "# print(input_matrix.shape, 576 * 22500)\n",
    "dit = DiT(hidden_size=576, num_heads=16, depth=28, length=length).to(device)\n",
    "get_model_memory_nolog(dit)\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "output_image = input_matrix.reshape(1,-1,576) # Reshape to (seq_len, batch_size, input_channel)  (N, T, D)\n",
    "t = torch.randint(0, diffusion.num_timesteps, (output_image.shape[0],), device=device)\n",
    "print(output_image.shape, output_image.shape[0] * output_image.shape[1] * output_image.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "output_image = dit(x=output_image,t=t,y=None)\n",
    "print(output_image.shape, output_image.shape[0] * output_image.shape[1] * output_image.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jxtnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
