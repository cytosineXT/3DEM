{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "torch.Size([1, 576, 25000]) 14400000\n",
      "初始化耗时0.2574s\n",
      "torch.Size([25000, 1, 576]) 14400000\n",
      "耗时0.0152s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 25000\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, hiddendim, tokenlength).to(device)  # batchsize channel 长\n",
    "\n",
    "#------------------------------模型初始化\n",
    "transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256),num_layers=6).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "#------------------------------数据流正文\n",
    "print(input_matrix.shape, hiddendim*tokenlength)\n",
    "print(f'初始化耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "x = input_matrix.reshape(tokenlength,1,-1) # Reshape to (seq_len, batch_size, input_channel)\n",
    "x = transformer_model(x)\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "torch.Size([1, 2500, 576]) 1440000\n",
      "初始化耗时0.0927s\n",
      "torch.Size([2500, 1, 576]) 1440000\n",
      "torch.Size([2500, 1, 576]) 1440000\n",
      "耗时0.0088s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 2500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "# input_matrix = torch.randn(1, hiddendim, tokenlength).to(device)  # batchsize dim length\n",
    "\n",
    "#------------------------------模型初始化\n",
    "# transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256, batch_first=True),num_layers=6).to(device)\n",
    "transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256,activation='silu'),num_layers=6).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "#------------------------------数据流正文\n",
    "print(input_matrix.shape, hiddendim*tokenlength)\n",
    "print(f'初始化耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "# x = input_matrix.reshape(1,tokenlength,-1) # Reshape to ( batch_size, seq_len, input_channel) (1,2500,576) batch length dim\n",
    "x = input_matrix.reshape(tokenlength,1,-1) # Reshape to (seq_len, batch_size, input_channel) (2500,1,576) length batch dim\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "\n",
    "x = transformer_model(x) #真正使用的时候\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "torch.Size([1, 22500, 576]) 12960000\n",
      "初始化耗时0.2944s\n",
      "torch.Size([22500, 1, 576]) 12960000\n",
      "torch.Size([22500, 1, 576]) 12960000\n",
      "耗时0.0196s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=22500): \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:1'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "# input_matrix = torch.randn(1, hiddendim, tokenlength).to(device)  # batchsize dim length\n",
    "\n",
    "#------------------------------模型初始化\n",
    "# transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256, batch_first=True),num_layers=6).to(device)\n",
    "transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256,activation='silu'),num_layers=6).to(device)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "#------------------------------数据流正文\n",
    "print(input_matrix.shape, hiddendim*tokenlength)\n",
    "print(f'初始化耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "# x = input_matrix.reshape(1,tokenlength,-1) # Reshape to ( batch_size, seq_len, input_channel) (1,2500,576) batch length dim\n",
    "x = input_matrix.reshape(tokenlength,1,-1) # Reshape to (seq_len, batch_size, input_channel) (2500,1,576) length batch dim\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "\n",
    "x = pe(x)\n",
    "x = transformer_model(x) #真正使用的时候\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试Transformer pooling 和query vector动手脚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "模型占用0.0000GB\n",
      "模型占用0.5132GB\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "模型占用0.0004GB\n",
      "torch.Size([2, 22500, 576]) 25920000\n",
      "耗时1.8190s\n",
      "torch.Size([22500, 2, 576]) 25920000\n",
      "进入Encoder\n",
      "1\n",
      "torch.Size([22500, 2, 576]) 25920000\n",
      "torch.Size([11250, 2, 576]) 12960000\n",
      "1\n",
      "torch.Size([11250, 2, 576]) 12960000\n",
      "torch.Size([5625, 2, 576]) 6480000\n",
      "1\n",
      "torch.Size([5625, 2, 576]) 6480000\n",
      "torch.Size([2812, 2, 576]) 3239424\n",
      "1\n",
      "torch.Size([2812, 2, 576]) 3239424\n",
      "torch.Size([1406, 2, 576]) 1619712\n",
      "1\n",
      "torch.Size([1406, 2, 576]) 1619712\n",
      "torch.Size([703, 2, 576]) 809856\n",
      "1\n",
      "torch.Size([703, 2, 576]) 809856\n",
      "torch.Size([351, 2, 576]) 404352\n",
      "耗时0.0109s\n",
      "进入bottleneck\n",
      "torch.Size([2, 576, 351]) 404352\n",
      "torch.Size([2, 1, 351]) 702\n",
      "torch.Size([2, 1, 388800]) 777600\n",
      "耗时0.0008s\n",
      "进入Decoder\n",
      "torch.Size([2, 16200, 48]) 1555200\n",
      "torch.Size([2, 64800, 24]) 3110400\n",
      "torch.Size([2, 259200, 12]) 6220800\n",
      "torch.Size([2, 259200, 12]) 6220800\n",
      "torch.Size([2, 1, 360, 720]) 720\n",
      "耗时0.7874s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Transformer输入：   (seq_len, batch_size, C)\n",
    "1DConv输入：        (batch_size, C, seq_len)\n",
    "2DConv输入：        (batch_size, C, H, W)\n",
    "Linear输入：        仅对最后一个维度从输入变成输出\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "def checksize(x):\n",
    "    print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "    return 1\n",
    "\n",
    "def toc(tic):\n",
    "    print(f'耗时{time.time() - tic:.4f}s')\n",
    "    tic = time.time()\n",
    "    return tic\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(2, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "\n",
    "#------------------------------模型初始化\n",
    "num_layers = 6\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "transformer_model = TransformerWithPooling(d_model=hiddendim, nhead=4, dim_feedforward=256, num_layers=num_layers, pool_size=pool_size, activation='silu').to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "conv1d1 = nn.Conv1d(576, 1, kernel_size=1, stride=1, dilation=1 ,padding=0).to(device)\n",
    "get_model_memory_nolog(conv1d1)\n",
    "fc1d1 = nn.Sequential(\n",
    "        nn.Linear(351, 351),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(351, 96*45*90),\n",
    "        nn.LayerNorm(96*45*90)).to(device)\n",
    "# fc1d1 = nn.Linear(351, 8*45*90)\n",
    "get_model_memory_nolog(fc1d1)\n",
    "swinunet = SwinTransformerSys(embed_dim=12,window_size=9).to(device)\n",
    "get_model_memory_nolog(swinunet)\n",
    "\n",
    "#------------------------------数据流正文----------------------------\n",
    "checksize(input_matrix)\n",
    "tic=toc(tic)\n",
    "x = input_matrix.reshape(tokenlength, input_matrix.shape[0], -1)  # Transformer输入：Reshape to (seq_len, batch_size, input_channel) (L B C)\n",
    "checksize(x)\n",
    "\n",
    "#---------------Transformer Encoder----------------\n",
    "print('进入Encoder')\n",
    "x = pe(x)\n",
    "x = transformer_model(x)  # 传入自定义的 Transformer 模型\n",
    "tic=toc(tic)\n",
    "\n",
    "#---------------conv1d+fc bottleneck---------------\n",
    "print(\"进入bottleneck\")\n",
    "x = x.reshape(x.shape[1], x.shape[2], -1)  # 1DConv输入：Reshape to (batch_size, input_channel, seq_len)\n",
    "checksize(x)\n",
    "x = conv1d1(x)\n",
    "checksize(x)\n",
    "x = fc1d1(x)\n",
    "checksize(x)\n",
    "tic=toc(tic)\n",
    "\n",
    "#-------------SwinTransformer Decoder--------------\n",
    "print(\"进入Decoder\")\n",
    "x = x.reshape(x.shape[0],45*90,-1)\n",
    "x = swinunet(x)\n",
    "checksize(x)\n",
    "tic=toc(tic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jxtnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
