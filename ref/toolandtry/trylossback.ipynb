{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0606GB\n",
      "模型占用0.0000GB\n",
      "模型占用0.0348GB\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "模型占用0.0004GB\n",
      "torch.Size([6, 22500, 576]) 77760000\n",
      "torch.Size([22500, 6, 576]) 77760000\n",
      "进入Encoder\n",
      "torch.Size([11250, 6, 576]) 38880000\n",
      "torch.Size([5625, 6, 576]) 19440000\n",
      "torch.Size([2812, 6, 576]) 9718272\n",
      "torch.Size([1406, 6, 576]) 4859136\n",
      "torch.Size([703, 6, 576]) 2429568\n",
      "torch.Size([351, 6, 576]) 1213056\n",
      "torch.Size([175, 6, 576]) 604800\n",
      "torch.Size([87, 6, 576]) 300672\n",
      "torch.Size([43, 6, 576]) 148608\n",
      "torch.Size([21, 6, 576]) 72576\n",
      "进入bottleneck\n",
      "torch.Size([6, 576, 21]) 72576\n",
      "torch.Size([6, 1, 21]) 126\n",
      "torch.Size([6, 1, 388800]) 2332800\n",
      "进入Decoder\n",
      "torch.Size([6, 4050, 96]) 2332800\n",
      "torch.Size([6, 16200, 48]) 4665600\n",
      "torch.Size([6, 64800, 24]) 9331200\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 1, 360, 720]) 1555200\n",
      "准备及推理时间 1.245736 seconds.\n",
      "反传时间 1.899117 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Transformer输入：   (seq_len, batch_size, C)\n",
    "1DConv输入：        (batch_size, C, seq_len)\n",
    "2DConv输入：        (batch_size, C, H, W)\n",
    "Linear输入：        仅对最后一个维度从输入变成输出\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss\n",
    "def checksize(x):\n",
    "    1\n",
    "    print(x.shape, torch.prod(torch.tensor(x.shape)).item())\n",
    "\n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(6, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "out_dim = 12\n",
    "#------------------------------模型初始化\n",
    "num_layers = 10\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "transformer_model = TransformerWithPooling(d_model=hiddendim, nhead=4, dim_feedforward=256, num_layers=num_layers, pool_size=pool_size, activation='silu').to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "conv1d1 = nn.Conv1d(576, 1, kernel_size=1, stride=1, dilation=1 ,padding=0).to(device)\n",
    "get_model_memory_nolog(conv1d1)\n",
    "fc1d1 = nn.Sequential(\n",
    "        nn.Linear(21, 21),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(21, out_dim*8*45*90),\n",
    "        nn.LayerNorm(out_dim*8*45*90)).to(device)\n",
    "# fc1d1 = nn.Linear(351, 8*45*90)\n",
    "get_model_memory_nolog(fc1d1)\n",
    "swinunet = SwinTransformerSys(embed_dim=out_dim,window_size=9).to(device)\n",
    "get_model_memory_nolog(swinunet)\n",
    "\n",
    "#------------------------------数据流正文----------------------------\n",
    "checksize(input_matrix)\n",
    "# tic=toc(tic)\n",
    "x = input_matrix.reshape(tokenlength, input_matrix.shape[0], -1)  # Transformer输入：Reshape to (seq_len, batch_size, input_channel) (L B C)\n",
    "checksize(x)\n",
    "\n",
    "#---------------Transformer Encoder----------------\n",
    "print('进入Encoder')\n",
    "x = pe(x)\n",
    "x = transformer_model(x)  # 传入自定义的 Transformer 模型\n",
    "# tic=toc(tic)\n",
    "\n",
    "#---------------conv1d+fc bottleneck---------------\n",
    "print(\"进入bottleneck\")\n",
    "x = x.reshape(x.shape[1], x.shape[2], -1)  # 1DConv输入：Reshape to (batch_size, input_channel, seq_len)\n",
    "checksize(x)\n",
    "x = conv1d1(x)\n",
    "checksize(x)\n",
    "x = fc1d1(x)\n",
    "checksize(x)\n",
    "# tic=toc(tic)\n",
    "\n",
    "#-------------SwinTransformer Decoder--------------\n",
    "print(\"进入Decoder\")\n",
    "x = x.reshape(x.shape[0],45*90,-1)\n",
    "checksize(x)\n",
    "x = swinunet(x)\n",
    "checksize(x)\n",
    "t.toc('准备及推理时间',restart=True)\n",
    "\n",
    "l1loss = TVL1Loss(beta=1.0)\n",
    "gt = torch.randn(6, 1, 360, 720).to(device)\n",
    "loss = l1loss(x,gt).mean()\n",
    "# print(loss)\n",
    "loss.backward()\n",
    "t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8层encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0485GB\n",
      "模型占用0.0000GB\n",
      "模型占用0.1304GB\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "模型占用0.0004GB\n",
      "torch.Size([6, 22500, 576]) 77760000\n",
      "torch.Size([22500, 6, 576]) 77760000\n",
      "进入Encoder\n",
      "torch.Size([11250, 6, 576]) 38880000\n",
      "torch.Size([5625, 6, 576]) 19440000\n",
      "torch.Size([2812, 6, 576]) 9718272\n",
      "torch.Size([1406, 6, 576]) 4859136\n",
      "torch.Size([703, 6, 576]) 2429568\n",
      "torch.Size([351, 6, 576]) 1213056\n",
      "torch.Size([175, 6, 576]) 604800\n",
      "torch.Size([87, 6, 576]) 300672\n",
      "进入bottleneck\n",
      "torch.Size([6, 576, 87]) 300672\n",
      "torch.Size([6, 1, 87]) 522\n",
      "torch.Size([6, 1, 388800]) 2332800\n",
      "进入Decoder\n",
      "torch.Size([6, 4050, 96]) 2332800\n",
      "torch.Size([6, 16200, 48]) 4665600\n",
      "torch.Size([6, 64800, 24]) 9331200\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 1, 360, 720]) 1555200\n",
      "准备及推理时间 1.423010 seconds.\n",
      "反传时间 1.887860 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Transformer输入：   (seq_len, batch_size, C)\n",
    "1DConv输入：        (batch_size, C, seq_len)\n",
    "2DConv输入：        (batch_size, C, H, W)\n",
    "Linear输入：        仅对最后一个维度从输入变成输出\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss\n",
    "def checksize(x):\n",
    "    1\n",
    "    print(x.shape, torch.prod(torch.tensor(x.shape)).item())\n",
    "\n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(6, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "out_dim = 12\n",
    "#------------------------------模型初始化\n",
    "num_layers = 8\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "transformer_model = TransformerWithPooling(d_model=hiddendim, nhead=4, dim_feedforward=256, num_layers=num_layers, pool_size=pool_size, activation='silu').to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "conv1d1 = nn.Conv1d(576, 1, kernel_size=1, stride=1, dilation=1 ,padding=0).to(device)\n",
    "get_model_memory_nolog(conv1d1)\n",
    "fc1d1 = nn.Sequential(\n",
    "        nn.Linear(87, 87),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(87, out_dim*8*45*90),\n",
    "        nn.LayerNorm(out_dim*8*45*90)).to(device)\n",
    "# fc1d1 = nn.Linear(351, 8*45*90)\n",
    "get_model_memory_nolog(fc1d1)\n",
    "swinunet = SwinTransformerSys(embed_dim=out_dim,window_size=9).to(device)\n",
    "get_model_memory_nolog(swinunet)\n",
    "\n",
    "#------------------------------数据流正文----------------------------\n",
    "checksize(input_matrix)\n",
    "# tic=toc(tic)\n",
    "x = input_matrix.reshape(tokenlength, input_matrix.shape[0], -1)  # Transformer输入：Reshape to (seq_len, batch_size, input_channel) (L B C)\n",
    "checksize(x)\n",
    "\n",
    "#---------------Transformer Encoder----------------\n",
    "print('进入Encoder')\n",
    "x = pe(x)\n",
    "x = transformer_model(x)  # 传入自定义的 Transformer 模型\n",
    "# tic=toc(tic)\n",
    "\n",
    "#---------------conv1d+fc bottleneck---------------\n",
    "print(\"进入bottleneck\")\n",
    "x = x.reshape(x.shape[1], x.shape[2], -1)  # 1DConv输入：Reshape to (batch_size, input_channel, seq_len)\n",
    "checksize(x)\n",
    "x = conv1d1(x)\n",
    "checksize(x)\n",
    "x = fc1d1(x)\n",
    "checksize(x)\n",
    "# tic=toc(tic)\n",
    "\n",
    "#-------------SwinTransformer Decoder--------------\n",
    "print(\"进入Decoder\")\n",
    "x = x.reshape(x.shape[0],45*90,-1)\n",
    "checksize(x)\n",
    "x = swinunet(x)\n",
    "checksize(x)\n",
    "t.toc('准备及推理时间',restart=True)\n",
    "\n",
    "l1loss = TVL1Loss(beta=1.0)\n",
    "gt = torch.randn(6, 1, 360, 720).to(device)\n",
    "loss = l1loss(x,gt).mean()\n",
    "# print(loss)\n",
    "loss.backward()\n",
    "t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4层encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0243GB\n",
      "模型占用0.0000GB\n",
      "模型占用2.0482GB\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "模型占用0.0004GB\n",
      "torch.Size([6, 22500, 576]) 77760000\n",
      "torch.Size([22500, 6, 576]) 77760000\n",
      "进入Encoder\n",
      "torch.Size([11250, 6, 576]) 38880000\n",
      "torch.Size([5625, 6, 576]) 19440000\n",
      "torch.Size([2812, 6, 576]) 9718272\n",
      "torch.Size([1406, 6, 576]) 4859136\n",
      "进入bottleneck\n",
      "torch.Size([6, 576, 1406]) 4859136\n",
      "torch.Size([6, 1, 1406]) 8436\n",
      "torch.Size([6, 1, 388800]) 2332800\n",
      "进入Decoder\n",
      "torch.Size([6, 4050, 96]) 2332800\n",
      "torch.Size([6, 16200, 48]) 4665600\n",
      "torch.Size([6, 64800, 24]) 9331200\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 1, 360, 720]) 1555200\n",
      "准备及推理时间 5.630191 seconds.\n",
      "反传时间 1.879369 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Transformer输入：   (seq_len, batch_size, C)\n",
    "1DConv输入：        (batch_size, C, seq_len)\n",
    "2DConv输入：        (batch_size, C, H, W)\n",
    "Linear输入：        仅对最后一个维度从输入变成输出\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss\n",
    "def checksize(x):\n",
    "    1\n",
    "    print(x.shape, torch.prod(torch.tensor(x.shape)).item())\n",
    "\n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(6, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "out_dim = 12\n",
    "#------------------------------模型初始化\n",
    "num_layers = 8\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "transformer_model = TransformerWithPooling(d_model=hiddendim, nhead=4, dim_feedforward=256, num_layers=num_layers, pool_size=pool_size, activation='silu').to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "conv1d1 = nn.Conv1d(576, 1, kernel_size=1, stride=1, dilation=1 ,padding=0).to(device)\n",
    "get_model_memory_nolog(conv1d1)\n",
    "fc1d1 = nn.Sequential(\n",
    "        nn.Linear(1406, 1406),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(1406, out_dim*8*45*90),\n",
    "        nn.LayerNorm(out_dim*8*45*90)).to(device)\n",
    "# fc1d1 = nn.Linear(351, 8*45*90)\n",
    "get_model_memory_nolog(fc1d1)\n",
    "swinunet = SwinTransformerSys(embed_dim=out_dim,window_size=9).to(device)\n",
    "get_model_memory_nolog(swinunet)\n",
    "\n",
    "#------------------------------数据流正文----------------------------\n",
    "checksize(input_matrix)\n",
    "# tic=toc(tic)\n",
    "x = input_matrix.reshape(tokenlength, input_matrix.shape[0], -1)  # Transformer输入：Reshape to (seq_len, batch_size, input_channel) (L B C)\n",
    "checksize(x)\n",
    "\n",
    "#---------------Transformer Encoder----------------\n",
    "print('进入Encoder')\n",
    "x = pe(x)\n",
    "x = transformer_model(x)  # 传入自定义的 Transformer 模型\n",
    "# tic=toc(tic)\n",
    "\n",
    "#---------------conv1d+fc bottleneck---------------\n",
    "print(\"进入bottleneck\")\n",
    "x = x.reshape(x.shape[1], x.shape[2], -1)  # 1DConv输入：Reshape to (batch_size, input_channel, seq_len)\n",
    "checksize(x)\n",
    "x = conv1d1(x)\n",
    "checksize(x)\n",
    "x = fc1d1(x)\n",
    "checksize(x)\n",
    "# tic=toc(tic)\n",
    "\n",
    "#-------------SwinTransformer Decoder--------------\n",
    "print(\"进入Decoder\")\n",
    "x = x.reshape(x.shape[0],45*90,-1)\n",
    "checksize(x)\n",
    "x = swinunet(x)\n",
    "checksize(x)\n",
    "t.toc('准备及推理时间',restart=True)\n",
    "\n",
    "l1loss = TVL1Loss(beta=1.0)\n",
    "gt = torch.randn(6, 1, 360, 720).to(device)\n",
    "loss = l1loss(x,gt).mean()\n",
    "# print(loss)\n",
    "loss.backward()\n",
    "t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始6层encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "模型占用0.0000GB\n",
      "模型占用0.5132GB\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "模型占用0.0004GB\n",
      "torch.Size([6, 22500, 576]) 77760000\n",
      "torch.Size([22500, 6, 576]) 77760000\n",
      "进入Encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangxiaotian/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11250, 6, 576]) 38880000\n",
      "torch.Size([5625, 6, 576]) 19440000\n",
      "torch.Size([2812, 6, 576]) 9718272\n",
      "torch.Size([1406, 6, 576]) 4859136\n",
      "torch.Size([703, 6, 576]) 2429568\n",
      "torch.Size([351, 6, 576]) 1213056\n",
      "进入bottleneck\n",
      "torch.Size([6, 576, 351]) 1213056\n",
      "torch.Size([6, 1, 351]) 2106\n",
      "torch.Size([6, 1, 388800]) 2332800\n",
      "进入Decoder\n",
      "torch.Size([6, 4050, 96]) 2332800\n",
      "torch.Size([6, 16200, 48]) 4665600\n",
      "torch.Size([6, 64800, 24]) 9331200\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 259200, 12]) 18662400\n",
      "torch.Size([6, 1, 360, 720]) 1555200\n",
      "准备及推理时间 4.055781 seconds.\n",
      "反传时间 1.303007 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Transformer输入：   (seq_len, batch_size, C)\n",
    "1DConv输入：        (batch_size, C, seq_len)\n",
    "2DConv输入：        (batch_size, C, H, W)\n",
    "Linear输入：        仅对最后一个维度从输入变成输出\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss\n",
    "def checksize(x):\n",
    "    1\n",
    "    print(x.shape, torch.prod(torch.tensor(x.shape)).item())\n",
    "    \n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(6, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "out_dim = 12\n",
    "#------------------------------模型初始化\n",
    "num_layers = 6\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "transformer_model = TransformerWithPooling(d_model=hiddendim, nhead=4, dim_feedforward=256, num_layers=num_layers, pool_size=pool_size, activation='silu').to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "conv1d1 = nn.Conv1d(576, 1, kernel_size=1, stride=1, dilation=1 ,padding=0).to(device)\n",
    "get_model_memory_nolog(conv1d1)\n",
    "fc1d1 = nn.Sequential(\n",
    "        nn.Linear(351, 351),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(351, out_dim*8*45*90),\n",
    "        nn.LayerNorm(out_dim*8*45*90)).to(device)\n",
    "# fc1d1 = nn.Linear(351, 8*45*90)\n",
    "get_model_memory_nolog(fc1d1)\n",
    "swinunet = SwinTransformerSys(embed_dim=out_dim,window_size=9).to(device)\n",
    "get_model_memory_nolog(swinunet)\n",
    "\n",
    "#------------------------------数据流正文----------------------------\n",
    "checksize(input_matrix)\n",
    "# tic=toc(tic)\n",
    "x = input_matrix.reshape(tokenlength, input_matrix.shape[0], -1)  # Transformer输入：Reshape to (seq_len, batch_size, input_channel) (L B C)\n",
    "checksize(x)\n",
    "\n",
    "#---------------Transformer Encoder----------------\n",
    "print('进入Encoder')\n",
    "x = pe(x)\n",
    "x = transformer_model(x)  # 传入自定义的 Transformer 模型\n",
    "# tic=toc(tic)\n",
    "\n",
    "#---------------conv1d+fc bottleneck---------------\n",
    "print(\"进入bottleneck\")\n",
    "x = x.reshape(x.shape[1], x.shape[2], -1)  # 1DConv输入：Reshape to (batch_size, input_channel, seq_len)\n",
    "checksize(x)\n",
    "x = conv1d1(x)\n",
    "checksize(x)\n",
    "x = fc1d1(x)\n",
    "checksize(x)\n",
    "# tic=toc(tic)\n",
    "\n",
    "#-------------SwinTransformer Decoder--------------\n",
    "print(\"进入Decoder\")\n",
    "x = x.reshape(x.shape[0],45*90,-1)\n",
    "checksize(x)\n",
    "x = swinunet(x)\n",
    "checksize(x)\n",
    "t.toc('准备及推理时间',restart=True)\n",
    "\n",
    "l1loss = TVL1Loss(beta=1.0)\n",
    "gt = torch.randn(6, 1, 360, 720).to(device)\n",
    "loss = l1loss(x,gt).mean()\n",
    "# print(loss)\n",
    "loss.backward()\n",
    "t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种拆分之后，还是应该loss.backward那四秒，只不过闪转腾挪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  初始化开始 27.228296 seconds.\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "  初始化结束 1.454404 seconds.\n",
      "准备时间 1.652453 seconds.\n",
      "-----------------------\n",
      "开始时间 0.001173 seconds.\n",
      "刚要进网络时间 0.236386 seconds.\n",
      "  刚进来 0.242952 seconds.\n",
      "  encoder 0.010540 seconds.\n",
      "  decoder 1.746488 seconds.\n",
      "  后处理 0.130223 seconds.\n",
      "推理时间 1.888701 seconds.\n",
      "反传时间 0.041992 seconds.\n",
      "-----------------------\n",
      "开始时间 0.001471 seconds.\n",
      "刚要进网络时间 4.136201 seconds.\n",
      "  刚进来 4.181268 seconds.\n",
      "  encoder 0.009690 seconds.\n",
      "  decoder 1.754822 seconds.\n",
      "  后处理 0.130295 seconds.\n",
      "推理时间 1.895566 seconds.\n",
      "反传时间 0.157857 seconds.\n",
      "-----------------------\n",
      "开始时间 0.002699 seconds.\n",
      "刚要进网络时间 4.039380 seconds.\n",
      "  刚进来 4.200757 seconds.\n",
      "  encoder 0.010183 seconds.\n",
      "  decoder 1.762697 seconds.\n",
      "  后处理 0.129596 seconds.\n",
      "推理时间 1.903291 seconds.\n",
      "反传时间 0.157922 seconds.\n",
      "-----------------------\n",
      "开始时间 0.003042 seconds.\n",
      "刚要进网络时间 4.052004 seconds.\n",
      "  刚进来 4.214041 seconds.\n",
      "  encoder 0.009582 seconds.\n",
      "  decoder 1.770646 seconds.\n",
      "  后处理 0.131164 seconds.\n",
      "推理时间 1.912247 seconds.\n",
      "反传时间 0.157241 seconds.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.utils import checksize\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss,MeshEncoderDecoder\n",
    "import os\n",
    "import re\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "from net.utils import meshRCSDataset, find_matching_files, process_files\n",
    "\n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "rcsdir = r'/home/jiangxiaotian/datasets/traintest' #T7920 Liang\n",
    "in_ems = []\n",
    "rcss = []\n",
    "for file in os.listdir(rcsdir):\n",
    "    if '.pt' in file:\n",
    "        # print(file)\n",
    "        plane, theta, phi, freq= re.search(r\"([a-zA-Z0-9]{4})_theta(\\d+)phi(\\d+)f(\\d.+).pt\", file).groups()\n",
    "        theta = int(theta)\n",
    "        phi = int(phi)\n",
    "        freq = float(freq)\n",
    "        in_em = [plane,theta,phi,freq]\n",
    "        rcs = torch.load(os.path.join(rcsdir,file))\n",
    "        in_ems.append(in_em)\n",
    "        rcss.append(rcs)\n",
    "dataset = meshRCSDataset(in_ems, rcss)\n",
    "dataloader = DataLoader.DataLoader(dataset, batch_size=6, shuffle=True, num_workers=0) #创建DataLoader迭代器\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "# input_matrix = torch.randn(6, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "decoder_outdim = 12\n",
    "#------------------------------模型初始化\n",
    "num_layers = 6\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "autoencoder = MeshEncoderDecoder(\n",
    "    num_discrete_coors = 128,\n",
    "    device= device,\n",
    "    paddingsize = 22500,\n",
    "    decoder_outdim=decoder_outdim #决定了decoder的size 12L 6M 3S\n",
    ").to(device)\n",
    "t.toc('准备时间',restart=True)\n",
    "\n",
    "for in_em1,rcs1 in dataloader:\n",
    "    print('-----------------------')\n",
    "    t.toc('开始时间',restart=True)\n",
    "    objlist , ptlist = find_matching_files(in_em1[0], \"./planes\")\n",
    "    planesur_faces, planesur_verts, _, geoinfo = process_files(objlist, device) #为了解决多batch变量长度不一样的问题 在这一步就已经padding到等长了\n",
    "    face = planesur_faces.clone().detach().to(device)\n",
    "    vert = planesur_verts.clone().detach().to(device)\n",
    "    # geo = geoinfo.clone().detach().to(device)\n",
    "    # inem = in_em1.clone().detach().to(device)\n",
    "    gt = rcs1.clone().detach().to(device)\n",
    "    t.toc('刚要进网络时间',restart=True)\n",
    "    loss, *_ = autoencoder( #这里使用网络，是进去跑了forward \n",
    "        vertices = vert,\n",
    "        faces = face, #torch.Size([batchsize, 33564, 3])\n",
    "        geoinfo = geoinfo, #[area, volume, scale]\n",
    "        in_em = in_em1,#.to(device)\n",
    "        GT = gt, #这里放真值\n",
    "        device = device,\n",
    "    )\n",
    "    t.toc('推理时间',restart=True)\n",
    "\n",
    "    # l1loss = TVL1Loss(beta=1.0)\n",
    "    # gt = torch.randn(6, 1, 360, 720).to(device)\n",
    "    # loss = l1loss(x,gt).mean()\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉也不是encoder的问题，就是反传的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  初始化开始 0.995110 seconds.\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "  初始化结束 1.287830 seconds.\n",
      "准备时间 2.284415 seconds.\n",
      "-----------------------\n",
      "开始时间 0.004077 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangxiaotian/anaconda3/envs/jxtnet/lib/python3.9/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  encoder 2.171665 seconds.\n",
      "  decoder 0.604875 seconds.\n",
      "  后处理 0.257600 seconds.\n",
      "推理时间 3.024815 seconds.\n",
      "反传时间 4.365017 seconds.\n",
      "-----------------------\n",
      "开始时间 0.006521 seconds.\n",
      "  encoder 4.697009 seconds.\n",
      "  decoder 1.744252 seconds.\n",
      "  后处理 0.130561 seconds.\n",
      "推理时间 2.200673 seconds.\n",
      "反传时间 0.156219 seconds.\n",
      "-----------------------\n",
      "开始时间 0.004101 seconds.\n",
      "  encoder 4.185405 seconds.\n",
      "  decoder 1.747972 seconds.\n",
      "  后处理 0.130603 seconds.\n",
      "推理时间 5.903793 seconds.\n",
      "反传时间 0.157812 seconds.\n",
      "-----------------------\n",
      "开始时间 0.007045 seconds.\n",
      "  encoder 4.202237 seconds.\n",
      "  decoder 1.755050 seconds.\n",
      "  后处理 0.129718 seconds.\n",
      "推理时间 5.922308 seconds.\n",
      "反传时间 0.157092 seconds.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.utils import checksize\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss,MeshEncoderDecoder\n",
    "import os\n",
    "import re\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "from net.utils import meshRCSDataset, find_matching_files, process_files\n",
    "\n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "rcsdir = r'/home/jiangxiaotian/datasets/traintest' #T7920 Liang\n",
    "in_ems = []\n",
    "rcss = []\n",
    "for file in os.listdir(rcsdir):\n",
    "    if '.pt' in file:\n",
    "        # print(file)\n",
    "        plane, theta, phi, freq= re.search(r\"([a-zA-Z0-9]{4})_theta(\\d+)phi(\\d+)f(\\d.+).pt\", file).groups()\n",
    "        theta = int(theta)\n",
    "        phi = int(phi)\n",
    "        freq = float(freq)\n",
    "        in_em = [plane,theta,phi,freq]\n",
    "        rcs = torch.load(os.path.join(rcsdir,file))\n",
    "        in_ems.append(in_em)\n",
    "        rcss.append(rcs)\n",
    "dataset = meshRCSDataset(in_ems, rcss)\n",
    "dataloader = DataLoader.DataLoader(dataset, batch_size=6, shuffle=True, num_workers=0) #创建DataLoader迭代器\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(6, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "decoder_outdim = 12\n",
    "#------------------------------模型初始化\n",
    "num_layers = 6\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "autoencoder = MeshEncoderDecoder( #这里实例化，是进去跑了init\n",
    "    num_discrete_coors = 128,\n",
    "    device= device,\n",
    "    paddingsize = 22500,\n",
    "    decoder_outdim=decoder_outdim #决定了decoder的size 12L 6M 3S\n",
    ").to(device)\n",
    "t.toc('准备时间',restart=True)\n",
    "\n",
    "for in_em1,rcs1 in dataloader:\n",
    "    print('-----------------------')\n",
    "    t.toc('开始时间',restart=True)\n",
    "    objlist , ptlist = find_matching_files(in_em1[0], \"./planes\")\n",
    "    planesur_faces, planesur_verts, _, geoinfo = process_files(objlist, device) #为了解决多batch变量长度不一样的问题 在这一步就已经padding到等长了\n",
    "    loss, *_ = autoencoder( #这里使用网络，是进去跑了forward \n",
    "        vertices = planesur_verts,\n",
    "        faces = planesur_faces, #torch.Size([batchsize, 33564, 3])\n",
    "        geoinfo = geoinfo, #[area, volume, scale]\n",
    "        in_em = in_em1,#.to(device)\n",
    "        GT = rcs1.to(device), #这里放真值\n",
    "        device = device,\n",
    "    )\n",
    "    t.toc('推理时间',restart=True)\n",
    "\n",
    "    # l1loss = TVL1Loss(beta=1.0)\n",
    "    # gt = torch.randn(6, 1, 360, 720).to(device)\n",
    "    # loss = l1loss(x,gt).mean()\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "torch.Size([5, 12996, 3]) 194940\n",
      "torch.Size([5, 22500, 576]) 64800000\n",
      "torch.Size([22500, 5, 576]) 64800000\n",
      "torch.Size([11250, 5, 576]) 32400000\n",
      "torch.Size([5625, 5, 576]) 16200000\n",
      "torch.Size([2812, 5, 576]) 8098560\n",
      "torch.Size([1406, 5, 576]) 4049280\n",
      "torch.Size([703, 5, 576]) 2024640\n",
      "torch.Size([351, 5, 576]) 1010880\n",
      "torch.Size([5, 576, 351]) 1010880\n",
      "torch.Size([5, 1, 351]) 1755\n",
      "torch.Size([5, 1, 388800]) 1944000\n",
      "torch.Size([5, 4050, 96]) 1944000\n",
      "torch.Size([5, 16200, 48]) 3888000\n",
      "torch.Size([5, 64800, 24]) 7776000\n",
      "torch.Size([5, 259200, 12]) 15552000\n",
      "torch.Size([5, 259200, 12]) 15552000\n",
      "torch.Size([5, 1, 360, 720]) 1296000\n",
      "准备及推理时间 4.102780 seconds.\n",
      "反传时间 0.043535 seconds.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.utils import checksize\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss,MeshEncoderDecoder\n",
    "import os\n",
    "import re\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "from net.utils import meshRCSDataset, find_matching_files, process_files\n",
    "\n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "rcsdir = r'/home/jiangxiaotian/datasets/traintest' #T7920 Liang\n",
    "in_ems = []\n",
    "rcss = []\n",
    "for file in os.listdir(rcsdir):\n",
    "    if '.pt' in file:\n",
    "        # print(file)\n",
    "        plane, theta, phi, freq= re.search(r\"([a-zA-Z0-9]{4})_theta(\\d+)phi(\\d+)f(\\d.+).pt\", file).groups()\n",
    "        theta = int(theta)\n",
    "        phi = int(phi)\n",
    "        freq = float(freq)\n",
    "        in_em = [plane,theta,phi,freq]\n",
    "        rcs = torch.load(os.path.join(rcsdir,file))\n",
    "        in_ems.append(in_em)\n",
    "        rcss.append(rcs)\n",
    "dataset = meshRCSDataset(in_ems, rcss)\n",
    "dataloader = DataLoader.DataLoader(dataset, batch_size=6, shuffle=True, num_workers=0) #创建DataLoader迭代器\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(6, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "decoder_outdim = 12\n",
    "#------------------------------模型初始化\n",
    "num_layers = 6\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "autoencoder = MeshEncoderDecoder( #这里实例化，是进去跑了init\n",
    "    num_discrete_coors = 128,\n",
    "    device= device,\n",
    "    paddingsize = 22500,\n",
    "    decoder_outdim=decoder_outdim #决定了decoder的size 12L 6M 3S\n",
    ").to(device)\n",
    "\n",
    "for in_em1,rcs1 in dataloader:\n",
    "        objlist , ptlist = find_matching_files(in_em1[0], \"./planes\")\n",
    "        planesur_faces, planesur_verts, _, geoinfo = process_files(objlist, device) #为了解决多batch变量长度不一样的问题 在这一步就已经padding到等长了\n",
    "        loss, *_ = autoencoder( #这里使用网络，是进去跑了forward \n",
    "            vertices = planesur_verts,\n",
    "            faces = planesur_faces, #torch.Size([batchsize, 33564, 3])\n",
    "            geoinfo = geoinfo, #[area, volume, scale]\n",
    "            in_em = in_em1,#.to(device)\n",
    "            GT = rcs1.to(device), #这里放真值\n",
    "            device = device,\n",
    "        )\n",
    "t.toc('准备及推理时间',restart=True)\n",
    "\n",
    "# l1loss = TVL1Loss(beta=1.0)\n",
    "# gt = torch.randn(6, 1, 360, 720).to(device)\n",
    "# loss = l1loss(x,gt).mean()\n",
    "# print(loss)\n",
    "loss.backward()\n",
    "t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "模型占用0.0000GB\n",
      "模型占用1.0259GB\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1000\n",
      "---final upsample expand_first---\n",
      "模型占用0.0016GB\n",
      "torch.Size([1, 22500, 576]) 12960000\n",
      "torch.Size([22500, 1, 576]) 12960000\n",
      "进入Encoder\n",
      "torch.Size([11250, 1, 576]) 6480000\n",
      "torch.Size([5625, 1, 576]) 3240000\n",
      "torch.Size([2812, 1, 576]) 1619712\n",
      "torch.Size([1406, 1, 576]) 809856\n",
      "torch.Size([703, 1, 576]) 404928\n",
      "torch.Size([351, 1, 576]) 202176\n",
      "进入bottleneck\n",
      "torch.Size([1, 576, 351]) 202176\n",
      "torch.Size([1, 1, 351]) 351\n",
      "torch.Size([1, 1, 777600]) 777600\n",
      "进入Decoder\n",
      "torch.Size([1, 4050, 192]) 777600\n",
      "torch.Size([1, 16200, 96]) 1555200\n",
      "torch.Size([1, 64800, 48]) 3110400\n",
      "torch.Size([1, 259200, 24]) 6220800\n",
      "torch.Size([1, 259200, 24]) 6220800\n",
      "torch.Size([1, 1, 360, 720]) 259200\n",
      "准备及推理时间 2.839354 seconds.\n",
      "反传时间 1.520759 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Transformer输入：   (seq_len, batch_size, C)\n",
    "1DConv输入：        (batch_size, C, seq_len)\n",
    "2DConv输入：        (batch_size, C, H, W)\n",
    "Linear输入：        仅对最后一个维度从输入变成输出\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.utils import checksize,toc\n",
    "from net.mytransformer import PositionalEncoding,TransformerWithPooling\n",
    "from net.myswinunet import SwinTransformerSys\n",
    "# import torch.nn.functional as F\n",
    "from pytictoc import TicToc\n",
    "from net.utils import get_model_memory_nolog\n",
    "from net.jxtnet_pureTrans import TVL1Loss\n",
    "\n",
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "out_dim = 24\n",
    "#------------------------------模型初始化\n",
    "num_layers = 6\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "transformer_model = TransformerWithPooling(d_model=hiddendim, nhead=4, dim_feedforward=256, num_layers=num_layers, pool_size=pool_size, activation='silu').to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "conv1d1 = nn.Conv1d(576, 1, kernel_size=1, stride=1, dilation=1 ,padding=0).to(device)\n",
    "get_model_memory_nolog(conv1d1)\n",
    "fc1d1 = nn.Sequential(\n",
    "        nn.Linear(351, 351),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(351, out_dim*8*45*90),\n",
    "        nn.LayerNorm(out_dim*8*45*90)).to(device)\n",
    "# fc1d1 = nn.Linear(351, 8*45*90)\n",
    "get_model_memory_nolog(fc1d1)\n",
    "swinunet = SwinTransformerSys(embed_dim=out_dim,window_size=9).to(device)\n",
    "get_model_memory_nolog(swinunet)\n",
    "\n",
    "#------------------------------数据流正文----------------------------\n",
    "checksize(input_matrix)\n",
    "# tic=toc(tic)\n",
    "x = input_matrix.reshape(tokenlength, input_matrix.shape[0], -1)  # Transformer输入：Reshape to (seq_len, batch_size, input_channel) (L B C)\n",
    "checksize(x)\n",
    "\n",
    "#---------------Transformer Encoder----------------\n",
    "print('进入Encoder')\n",
    "x = pe(x)\n",
    "x = transformer_model(x)  # 传入自定义的 Transformer 模型\n",
    "# tic=toc(tic)\n",
    "\n",
    "#---------------conv1d+fc bottleneck---------------\n",
    "print(\"进入bottleneck\")\n",
    "x = x.reshape(x.shape[1], x.shape[2], -1)  # 1DConv输入：Reshape to (batch_size, input_channel, seq_len)\n",
    "checksize(x)\n",
    "x = conv1d1(x)\n",
    "checksize(x)\n",
    "x = fc1d1(x)\n",
    "checksize(x)\n",
    "# tic=toc(tic)\n",
    "\n",
    "#-------------SwinTransformer Decoder--------------\n",
    "print(\"进入Decoder\")\n",
    "x = x.reshape(x.shape[0],45*90,-1)\n",
    "checksize(x)\n",
    "x = swinunet(x)\n",
    "checksize(x)\n",
    "t.toc('准备及推理时间',restart=True)\n",
    "\n",
    "l1loss = TVL1Loss(beta=1.0)\n",
    "gt = torch.randn(1, 1, 360, 720).to(device)\n",
    "loss = l1loss(x,gt)\n",
    "loss.backward()\n",
    "t.toc('反传时间',restart=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jxtnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
