{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "torch.Size([1, 576, 25000]) 14400000\n",
      "初始化耗时0.2574s\n",
      "torch.Size([25000, 1, 576]) 14400000\n",
      "耗时0.0152s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 25000\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, hiddendim, tokenlength).to(device)  # batchsize channel 长\n",
    "\n",
    "#------------------------------模型初始化\n",
    "transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256),num_layers=6).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "#------------------------------数据流正文\n",
    "print(input_matrix.shape, hiddendim*tokenlength)\n",
    "print(f'初始化耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "x = input_matrix.reshape(tokenlength,1,-1) # Reshape to (seq_len, batch_size, input_channel)\n",
    "x = transformer_model(x)\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "torch.Size([1, 2500, 576]) 1440000\n",
      "初始化耗时0.0927s\n",
      "torch.Size([2500, 1, 576]) 1440000\n",
      "torch.Size([2500, 1, 576]) 1440000\n",
      "耗时0.0088s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:0'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 2500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "# input_matrix = torch.randn(1, hiddendim, tokenlength).to(device)  # batchsize dim length\n",
    "\n",
    "#------------------------------模型初始化\n",
    "# transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256, batch_first=True),num_layers=6).to(device)\n",
    "transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256,activation='silu'),num_layers=6).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "#------------------------------数据流正文\n",
    "print(input_matrix.shape, hiddendim*tokenlength)\n",
    "print(f'初始化耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "# x = input_matrix.reshape(1,tokenlength,-1) # Reshape to ( batch_size, seq_len, input_channel) (1,2500,576) batch length dim\n",
    "x = input_matrix.reshape(tokenlength,1,-1) # Reshape to (seq_len, batch_size, input_channel) (2500,1,576) length batch dim\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "\n",
    "x = transformer_model(x) #真正使用的时候\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "torch.Size([1, 22500, 576]) 12960000\n",
      "初始化耗时0.2944s\n",
      "torch.Size([22500, 1, 576]) 12960000\n",
      "torch.Size([22500, 1, 576]) 12960000\n",
      "耗时0.0196s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=22500): \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:1'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 22500\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "# input_matrix = torch.randn(1, hiddendim, tokenlength).to(device)  # batchsize dim length\n",
    "\n",
    "#------------------------------模型初始化\n",
    "# transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256, batch_first=True),num_layers=6).to(device)\n",
    "transformer_model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hiddendim, nhead=4, dim_feedforward=256,activation='silu'),num_layers=6).to(device)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "#------------------------------数据流正文\n",
    "print(input_matrix.shape, hiddendim*tokenlength)\n",
    "print(f'初始化耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "# x = input_matrix.reshape(1,tokenlength,-1) # Reshape to ( batch_size, seq_len, input_channel) (1,2500,576) batch length dim\n",
    "x = input_matrix.reshape(tokenlength,1,-1) # Reshape to (seq_len, batch_size, input_channel) (2500,1,576) length batch dim\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "\n",
    "x = pe(x)\n",
    "x = transformer_model(x) #真正使用的时候\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试Transformer pooling 和query vector动手脚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型占用0.0364GB\n",
      "torch.Size([1, 25000, 576]) 14400000\n",
      "初始化耗时0.0001s\n",
      "torch.Size([25000, 1, 576]) 14400000\n",
      "torch.Size([25000, 1, 576]) 14400000\n",
      "1\n",
      "torch.Size([12500, 1, 576]) 7200000\n",
      "torch.Size([12500, 1, 576]) 7200000\n",
      "1\n",
      "torch.Size([6250, 1, 576]) 3600000\n",
      "torch.Size([6250, 1, 576]) 3600000\n",
      "1\n",
      "torch.Size([3125, 1, 576]) 1800000\n",
      "torch.Size([3125, 1, 576]) 1800000\n",
      "1\n",
      "torch.Size([1562, 1, 576]) 899712\n",
      "torch.Size([1562, 1, 576]) 899712\n",
      "1\n",
      "torch.Size([781, 1, 576]) 449856\n",
      "torch.Size([781, 1, 576]) 449856\n",
      "1\n",
      "torch.Size([390, 1, 576]) 224640\n",
      "torch.Size([390, 1, 576]) 224640\n",
      "耗时0.0083s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "from mytransformer import TransformerEncoder,TransformerEncoderLayer,PositionalEncoding,TransformerWithPooling\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from net.utils import get_model_memory_nolog\n",
    "tic = time.time()\n",
    "\n",
    "#------------------------------参数设置\n",
    "cudadevice = 'cuda:1'\n",
    "device = torch.device(cudadevice if torch.cuda.is_available() else \"cpu\")\n",
    "tokenlength = 25000\n",
    "hiddendim = 576\n",
    "input_matrix = torch.randn(1, tokenlength, hiddendim).to(device)  # batchsize length dim\n",
    "\n",
    "#------------------------------模型初始化\n",
    "num_layers = 6\n",
    "pool_size = 2  # 每次减少一半的序列长度\n",
    "\n",
    "transformer_model = TransformerWithPooling(d_model=hiddendim, nhead=4, dim_feedforward=256, num_layers=num_layers, pool_size=pool_size, activation='silu').to(device)\n",
    "pe = PositionalEncoding(d_model=hiddendim).to(device)\n",
    "get_model_memory_nolog(transformer_model)\n",
    "\n",
    "#------------------------------数据流正文\n",
    "tic = time.time()\n",
    "print(input_matrix.shape, hiddendim * tokenlength)\n",
    "print(f'初始化耗时{time.time() - tic:.4f}s')\n",
    "tic = time.time()\n",
    "\n",
    "x = input_matrix.reshape(tokenlength, 1, -1)  # Reshape to (seq_len, batch_size, input_channel)\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "\n",
    "x = pe(x)\n",
    "x = transformer_model(x)  # 传入自定义的 Transformer 模型\n",
    "print(x.shape, x.shape[0] * x.shape[1] * x.shape[2])\n",
    "print(f'耗时{time.time() - tic:.4f}s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jxtnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
